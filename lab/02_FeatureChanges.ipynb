{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a8724881-5f07-48f3-ba04-3866082e3eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "10793672-cbce-4f5c-8f76-a8a4e804ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.argv=[''] \n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "47e0d689-b3ad-46d3-8a95-e69442d830d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_DEVICES_VISIBLE']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1849461a-e49d-4bd8-aef2-6c48090d2a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/SymmNeRF\n"
     ]
    }
   ],
   "source": [
    "cd \"/root/SymmNeRF/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "16912e3d-f7f8-4dba-a59a-37d21b24b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt import config_parser\n",
    "\n",
    "parser = config_parser()\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = 'cuda'\n",
    "det = args.det\n",
    "lindisp = args.lindisp\n",
    "args.distributed = False\n",
    "args.local_feature_ch = 32\n",
    "args.local_rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5a2d06f9-371b-4240-a2b8-cd3e543cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_dict, create_training_dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from model import model_dict  \n",
    "from model.sample_ray import RaySampler \n",
    "from model.render_ray import render_rays\n",
    "\n",
    "from utils_lab_fct import * \n",
    "\n",
    "import setproctitle\n",
    "import torch \n",
    "import numpy as np \n",
    "import cv2\n",
    "from einops import rearrange \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setproctitle.setproctitle('[Gaetan. - SymmNeRF]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82ebdc-32b0-4789-b2b4-118c6960412a",
   "metadata": {},
   "source": [
    "### Build up a DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c3b3e0b0-72bc-429a-8041-241b74ac67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Training dataset: srns_dataset\n",
      "[Info] Set used: test\n",
      "SRNsDataset:  /data/datasets/srn_cars/cars_train\n"
     ]
    }
   ],
   "source": [
    "bs = 2\n",
    "\n",
    "train_dataset, train_sampler = create_training_dataset(args)\n",
    "train_loader = DataLoader(train_dataset,batch_size = bs , sampler = train_sampler)\n",
    "\n",
    "it = iter(train_loader)\n",
    "train_batch = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37803875-5f20-487e-8f0c-00041fa7d095",
   "metadata": {},
   "source": [
    "#### Model loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fd4b6ab0-531e-49ad-9aa9-e6888210793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] No ckpts found, training from scratch...\n"
     ]
    }
   ],
   "source": [
    "model = model_dict['hypernerf_symm_local'](args,ckpts_folder = \"\")#/root/SymmNeRF/logs/srns_dataset/cars/srns_cars/ckpts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6083cc-6161-4992-95da-d5af8792fefe",
   "metadata": {},
   "source": [
    "#### HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0d935188-a050-40bf-8ecf-6b1c784dd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some constants. \n",
    "nb_rays = 256\n",
    "nb_sampled_points_on_rays = 64\n",
    "\n",
    "# First 2 dim. of the feature map F \n",
    "W_F, H_F = 64, 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030eaadb-722e-4213-b7ae-1ba19b502624",
   "metadata": {},
   "source": [
    "#### Create a ray batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d8768ac7-59f3-4d16-9399-193cae10d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RaySampler class instantiation.\n",
    "ray_sampler = RaySampler(train_batch) \n",
    "\n",
    "# Get the corresponding source images and poses.\n",
    "src_imgs = ray_sampler.src_img     # [B, 3, 128,128]\n",
    "tgt_imgs = ray_sampler.render_imgs # [B,NV,3,128,128]\n",
    "poses = ray_sampler.render_poses   # [B, NV, 4, 4 ]\n",
    "\n",
    "# Build up a batch of ray. \n",
    "ray_batch = ray_sampler.random_sample(nb_rays,use_bbox = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542a656-8d79-4f02-b72b-00291d54e7a6",
   "metadata": {},
   "source": [
    "### Get the latent code from the Source Images. \n",
    "The .encode() method encodes the RGB source images. Correspond to the f() network in the main paper. \n",
    "Each code is a 256d vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d164d3dd-b692-4b2d-a4d8-329129590a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode(ray_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd043293-589d-45fb-be70-ca10a3428c34",
   "metadata": {},
   "source": [
    "### Render the rays. \n",
    "Considering the rays that were sampled on this batch, render them according to symmetry plane that is defined (points are expressed in the source camera viewpoint). \n",
    "Volume rendering is also perfomed as well as a last final step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d71aa09a-5a88-4ad1-9144-27b129c3bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 64, 64])\n",
      "torch.Size([2, 16384, 32])\n"
     ]
    }
   ],
   "source": [
    "from model.render_ray import sample_along_camera_ray, run_network, raw2outputs\n",
    "from model.nerf import run_nerf_symm_local\n",
    "from model.nerf_helpers import *\n",
    "from model.render_ray import get_symmetric_points_and_directions\n",
    "\n",
    "\n",
    "rays_o = ray_batch['rays_o']    # [B,256,3]\n",
    "rays_d = ray_batch['rays_d']    # [B,256,3]\n",
    "z_near = ray_batch['z_near']\n",
    "z_far = ray_batch['z_far']\n",
    "\n",
    "noise = False \n",
    "\n",
    "M = torch.tensor([[-1.,0.,0.],\n",
    "                  [0.,1.,0.],\n",
    "                  [0.,0.,1.]]).to(device)\n",
    "\n",
    "F = model.feature_net.latent\n",
    "print(F.shape)\n",
    "#### 1. Get the 3D points ray, the viewdir and z sampled dist and symmetrize it. \n",
    "pts, viewdirs, z_vals = sample_along_camera_ray(rays_o=rays_o,  # pts : [B,256,64,3] - viewdirs : [B,256,3] - z_vals : [B,256,64]\n",
    "                                                rays_d=rays_d,\n",
    "                                                z_near=z_near,\n",
    "                                                z_far=z_far,\n",
    "                                                device=device,\n",
    "                                                N_samples=nb_sampled_points_on_rays,\n",
    "                                                lindisp=lindisp, \n",
    "                                                det=det)\n",
    "\n",
    "pts_s,viewdirs_s = get_symmetric_points_and_directions(pts,viewdirs,M)\n",
    "\n",
    "#### 2. Get the NeRF weights according to the latent code z. \n",
    "nerf_coarse_layers = model.hypernetwork(z) \n",
    "\n",
    "### 3. Get a set of feature for experimental purpose ( F feature tensor, uv coordinates etc... ) \n",
    "ret_features = model.feature_net.index_experimental(pts, \n",
    "                                        ray_batch['src_pose'],\n",
    "                                        ray_batch['intrinsics'], \n",
    "                                        ray_batch['image_size'], \n",
    "                                        M,\n",
    "                                        noise)\n",
    "### 4. Retrieve f ans f_S \n",
    "f = ret_features['local_feature']\n",
    "f_S = ret_features['local_feature_symm']\n",
    "\n",
    "\n",
    "### 5. Define the NeRF models (one for each image in the batch since they all have a different latent code z). \n",
    "nerf_coarse = lambda x: run_nerf_symm_local(x, nerf_layers=nerf_coarse_layers, input_ch=model.input_ch,\n",
    "                                                input_ch_views=model.input_ch_views,\n",
    "                                                local_feature_ch=32)\n",
    "\n",
    "\n",
    "### 6. Do raw forward pass - rgb and alpha are not normalized correctly - Volume rendering was not performed yet. \n",
    "raw_coarse = run_network(pts,viewdirs,nerf_coarse,model.embed_fn,model.embeddirs_fn,f)\n",
    "\n",
    "raw_coarse_S = run_network(pts_s,viewdirs_s,nerf_coarse,model.embed_fn,model.embeddirs_fn,f_S)\n",
    "\n",
    "\n",
    "### 7. Perform the volume rendering twice (also for the symmetric)\n",
    "outputs_coarse = raw2outputs(raw_coarse,z_vals,ray_batch['rays_d'],device = device, raw_noise_std = 0.,white_bkgd = True)\n",
    "outputs_coarse_S = raw2outputs(raw_coarse_S,z_vals,ray_batch['rays_d'],device=device,raw_noise_std = 0.,white_bkgd = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6868d8-4b99-4cfb-b728-3388b649cb32",
   "metadata": {},
   "source": [
    "### Change the CNN feature network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aae4dd7c-af0c-40c2-b779-15c46a88feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model.feature_network import _resnet_symm_local, BasicBlock,ResNetSymmLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fd37e33-9b7a-4e3b-a80a-a43d5c2b95a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1092d05-bbb3-475d-bfef-a34f210a91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2,\n",
    "                               bias=False))\n",
    "                    # nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "36858f69-7dac-46f7-a1ab-0b70ef48fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _resnet_symm_local('resnet18',latent_dim = 256,block = BasicBlock, layers= [2,2,2,2], pretrained= False, progress= True)\n",
    "\n",
    "\n",
    "\n",
    "# 128,128,3 --> Input.\n",
    "# conv1 : 64,64,32\n",
    "# layer1 : 32,32,64\n",
    "# layer2 : 16,16,\n",
    "# 64,64,32\n",
    "# 32,32,64\n",
    "# 16,16,96\n",
    "\n",
    "# --> 64,64, 128+64 = 192. --> 64,64,64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "354eb5d3-4bad-4d2c-ba64-f3dffad45c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x after the first conv: torch.Size([1, 8, 64, 64])\n",
      "Shape of x after l1: torch.Size([1, 16, 64, 64])\n",
      "Shape of x after l2: torch.Size([1, 32, 32, 32])\n",
      "Shape of x after l3: torch.Size([1, 64, 16, 16])\n",
      "Shape of latent before: torch.Size([1, 120, 64, 64])\n",
      "Shape of latent after: torch.Size([1, 32, 64, 64])\n",
      "120\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "model = ResNetSymmLocalTest(block = BasicBlock, layers = [2,2,2,2])\n",
    "\n",
    "x = torch.rand(1,3,128,128).to('cpu')\n",
    "\n",
    "y = model(x) \n",
    "print(64+32+16+8)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "79f8e55b-22d3-4ec2-9916-4a65ee53e944",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_274956/2569204972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetSymmLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "model = ResNetSymmLocal(block = BasicBlock, layers = [2,2,2,2], **kwargs)\n",
    "x = torch.rand(1,3,128,128).to('cpu')\n",
    "y = model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "50d22c08-c349-4726-bb08-1b9e4ff30264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSymmLocalTest(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1,\n",
    "                 width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, index_interp='bilinear',\n",
    "                 index_padding='border', upsample_interp='bilinear', feature_scale=1.0, use_first_pool=False):\n",
    "        super().__init__()\n",
    "        # feature_scale factor to scale all latent by. Useful (<1) if image is extremely large, to fit in memory.\n",
    "        self.feature_scale = feature_scale\n",
    "        self.use_first_pool = use_first_pool\n",
    "        self.index_interp = index_interp\n",
    "        self.index_padding = index_padding\n",
    "        self.upsample_interp = upsample_interp\n",
    "\n",
    "        self.register_buffer(\"latent\", torch.empty(1, 1, 1, 1), persistent=False)\n",
    "        self.register_buffer(\"latent_scaling\", torch.empty(2, dtype=torch.float32), persistent=False)\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 8\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Last \n",
    "        self.conv_last = nn.Conv2d(120, 32, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn_last = norm_layer(32)\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 128, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.fc = nn.Linear(128,256)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "    \n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        if self.feature_scale != 1.0:\n",
    "            x = F.interpolate(x, scale_factor=self.feature_scale,\n",
    "                              mode='bilinear' if self.feature_scale > 1.0 else 'area',\n",
    "                              align_corners=True if self.feature_scale > 1.0 else False,\n",
    "                              recompute_scale_factor=True)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        print(f'Shape of x after the first conv: {x.shape}')\n",
    "        latents = [x]\n",
    "        if self.use_first_pool:\n",
    "            x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        print(f'Shape of x after l1: {x.shape}')\n",
    "        latents.append(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        print(f'Shape of x after l2: {x.shape}')\n",
    "        latents.append(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        print(f\"Shape of x after l3: {x.shape}\")\n",
    "        latents.append(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "\n",
    "      \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        align_corners = False if self.index_interp == 'nearest' else True\n",
    "        latent_sz = latents[0].shape[-2:]\n",
    "        \n",
    "        for i in range(len(latents)):\n",
    "            latents[i] = F.interpolate(latents[i], latent_sz,\n",
    "                                       mode=self.upsample_interp,\n",
    "                                       align_corners=align_corners)\n",
    "            \n",
    "        latents = torch.cat(latents,dim=1)\n",
    "        latents = self.conv_last(latents)\n",
    "        latents = self.bn_last(latents)\n",
    "        \n",
    "        self.latent = self.relu(latents)\n",
    "        \n",
    "        #self.latent = torch.cat(latents, dim=1)\n",
    "        print(f'Shape of latent after: {self.latent.shape}')\n",
    "        self.latent_scaling[0] = self.latent.shape[-1]\n",
    "        self.latent_scaling[1] = self.latent.shape[-2]\n",
    "        self.latent_scaling = self.latent_scaling / (self.latent_scaling - 1) * 2.0\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7c6b4-aa7a-4b72-b131-d0f50af9b0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
